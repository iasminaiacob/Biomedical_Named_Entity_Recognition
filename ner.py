# -*- coding: utf-8 -*-
"""NER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vILaU-65yhbrDGaqf4ziDYegMjqXsw73

Biomedical Named Entity Recognition
"""

!git clone https://huggingface.co/datasets/masaenger/bc5cdr

!pip install seqeval gradio transformers

import pandas as pd

train_df = pd.read_parquet('/content/bc5cdr/bc5cdr_source/train-00000-of-00001.parquet')
val_df = pd.read_parquet('/content/bc5cdr/bc5cdr_source/validation-00000-of-00001.parquet')
test_df = pd.read_parquet('/content/bc5cdr/bc5cdr_source/test-00000-of-00001.parquet')

print(f"Train: {train_df.shape}")
print(f"Validation: {val_df.shape}")
print(f"Test: {test_df.shape}")

print(train_df.head())

#unpack first row
sample = train_df['passages'][0]
text_parts = []
entities = []

for section in sample:
    #collect text
    text_parts.append(section['text'])

    #collect entities
    for ent in section['entities']:
        entities.append({
            'text': ent['text'],
            'start': ent['offsets'][0][0],
            'end': ent['offsets'][0][1],
            'type': ent['type']
        })

#combine all text (title + abstract)
full_text = ' '.join(text_parts)
print("Full text:", full_text)
print("Entities:", entities)

from transformers import AutoTokenizer

#load BioBERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-base-cased-v1.1")

def align_labels_with_tokens(text, entities, tokenizer):
    #create a char-level label list
    char_labels = ['O'] * len(text)
    for ent in entities:
        start = int(ent['start'])
        end = int(ent['end'])
        ent_type = ent['type']
        char_labels[start] = f'B-{ent_type}'
        for i in range(start + 1, end):
            char_labels[i] = f'I-{ent_type}'

    #tokenize text with offsets
    tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True)
    labels = []

    for idx, (start, end) in enumerate(tokenized['offset_mapping']):
        if start == end:
            #special tokens like [CLS], [SEP]
            labels.append('O')
        else:
            token_label = char_labels[start]
            labels.append(token_label)

    return tokenized, labels

#example
tokenized_input, bio_labels = align_labels_with_tokens(full_text, entities, tokenizer)

print("Tokens:", tokenizer.convert_ids_to_tokens(tokenized_input['input_ids']))
print("BIO Labels:", bio_labels)

#define label list
label_list = ['O', 'B-Chemical', 'I-Chemical', 'B-Disease', 'I-Disease']
label_to_id = {label: idx for idx, label in enumerate(label_list)}
id2label = {idx: label for idx, label in enumerate(label_list)}

def encode_sample(text, entities, tokenizer, max_length=512):
    tokenized, bio_labels = align_labels_with_tokens(text, entities, tokenizer)
    label_ids = [label_to_id[label] if label in label_to_id else label_to_id['O'] for label in bio_labels]

    #pad or truncate labels to match max_length
    if len(label_ids) > max_length:
        label_ids = label_ids[:max_length]
    else:
        label_ids = label_ids + [label_to_id['O']] * (max_length - len(label_ids))

    encoded = tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )

    return {
        'input_ids': encoded['input_ids'].squeeze(),  # remove batch dim
        'attention_mask': encoded['attention_mask'].squeeze(),
        'labels': torch.tensor(label_ids)
    }

#build dataset
from torch.utils.data import Dataset
import torch

class NERDataset(Dataset):
    def __init__(self, df, tokenizer):
        self.samples = []
        for row in df['passages']:
            #flatten all sections
            text_parts = [p['text'] for p in row]
            full_text = ' '.join(text_parts)
            entities = []
            for p in row:
                for ent in p['entities']:
                    entities.append({
                        'text': ent['text'],
                        'start': ent['offsets'][0][0],
                        'end': ent['offsets'][0][1],
                        'type': ent['type']
                    })
            encoded = encode_sample(full_text, entities, tokenizer)
            self.samples.append(encoded)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

train_dataset = NERDataset(train_df, tokenizer)
val_dataset = NERDataset(val_df, tokenizer)

import numpy as np
from seqeval.metrics import precision_score, recall_score, f1_score, classification_report

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    true_labels = [
        [label_list[label_id] for (label_id, label_mask) in zip(label_row, label_row) if label_id != -100]
        for label_row in labels
    ]
    true_predictions = [
        [label_list[pred_id] for (pred_id, label_id) in zip(pred_row, label_row) if label_id != -100]
        for pred_row, label_row in zip(predictions, labels)
    ]

    precision = precision_score(true_labels, true_predictions)
    recall = recall_score(true_labels, true_predictions)
    f1 = f1_score(true_labels, true_predictions)

    print(classification_report(true_labels, true_predictions))

    return {
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

#trainer
from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

#weights
class_counts = [100000, 5238, 5238, 4204, 4204]  #[O, B-Chemical, I-Chemical, B-Disease, I-Disease]
total = sum(class_counts)
class_weights = [total / c for c in class_counts]
class_weights = torch.tensor(class_weights)

model = AutoModelForTokenClassification.from_pretrained(
    "dmis-lab/biobert-base-cased-v1.1",
    num_labels=len(label_list),
)

#inject weights into loss function
model.config.problem_type = "single_label_classification"
model.loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)

training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=10,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.save_model("./bio_ner_model")
tokenizer.save_pretrained("./bio_ner_model")

#eval
metrics = trainer.evaluate()
print(metrics)

import gradio as gr
from transformers import pipeline

def clean_bio_predictions(entities):
    cleaned = []
    prev_label = 'O'
    for ent in entities:
        label = ent['entity']
        if label.startswith('I-') and prev_label == 'O':
            label = 'B-' + label[2:]
        ent['entity'] = label
        cleaned.append(ent)
        prev_label = label
    return cleaned

def merge_entities(entities):
    merged = []
    current_entity = None

    for ent in entities:
        if ent['entity'].startswith('B-'):
            #start of a new entity
            if current_entity:
                merged.append(current_entity)
            current_entity = {
                'entity': ent['entity'][2:],  #remove B-/I-
                'word': ent['word'].lstrip('##'),
                'score': ent['score'],
                'start': ent['start'],
                'end': ent['end']
            }
        elif ent['entity'].startswith('I-') and current_entity:
            #continuation of the current entity
            current_entity['word'] += ent['word'].lstrip('##')
            current_entity['score'] = max(current_entity['score'], ent['score'])
            current_entity['end'] = ent['end']
        else:
            #outside entity or unexpected I- without B-
            if current_entity:
                merged.append(current_entity)
                current_entity = None

    #append any remaining entity
    if current_entity:
        merged.append(current_entity)

    return merged

#interactive demo

model_path = "./bio_ner_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForTokenClassification.from_pretrained(model_path)

ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

def ner_predict(text):
    results = ner_pipeline(text)
    entities = []
    for r in results:
        entities.append({
            'entity': id2label[int(r['entity_group'].split('_')[-1])],
            'word': r['word'],
            'score': float(r['score']),
            'start': r['start'],
            'end': r['end']
        })

    entities = clean_bio_predictions(entities)

    merged_entities = merge_entities(entities)

    return {
        'merged_entities': merged_entities,
        'detailed_per_token': entities
    }

demo = gr.Interface(fn=ner_predict, inputs="text", outputs="json", title="Biomedical NER Demo")
demo.launch()

#error check

#model predictions on validation set
outputs = trainer.predict(val_dataset)
predictions = np.argmax(outputs.predictions, axis=2)
labels = outputs.label_ids

#map label IDs to label names
true_labels = [
    [label_list[label_id] for label_id in label_row if label_id != -100]
    for label_row in labels
]
predicted_labels = [
    [label_list[pred_id] for pred_id, label_id in zip(pred_row, label_row) if label_id != -100]
    for pred_row, label_row in zip(predictions, labels)
]

for i in range(5):  # look at first 5 samples
    print(f"\nExample {i}")
    print("TRUE :", true_labels[i])
    print("PRED :", predicted_labels[i])

    if true_labels[i] != predicted_labels[i]:
        print(">> MISMATCH FOUND!")